<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Mac 录制音频，波形图的绘制]]></title>
    <url>%2F2017%2F11%2F08%2Fdraw-voice-wave-view%2F</url>
    <content type="text"><![CDATA[开篇废话不多说，先上效果图。 上面一闪一闪的大概是没录好吧。🙄 需求分析首先分析这段动画的过程： 第一阶段：点击图标后，图标像泡泡一样收缩放大； 第二阶段：图标缩小，同时向下方移动； 第三阶段：波形图绘制，随着数据的增多，波形图向左移动，同时视图的宽度越来越长 第四阶段：视图宽度达到一个阈值后不再增大，波形图继续向左移动，超出左侧范围的部分不再显示。 PS. 波形图是上下对称的哦。 实现思路首先看动画，前两个阶段的动画有先后顺序，第二阶段的两个动画同时发生，可以用动画组来做。 然后，第二、第三阶段之间的节点，可以看到图标刚好缩小到波形视图的高度后，波形视图出现，如果分为两个视图，总有一个生硬的过渡过程，为了达到一个流畅的视觉效果，可以使用一个视图来做。 最后，仔细看波形图，新数据是从右侧进入的，由于这是个录音的过程，因此数据是在不断增多的，可以用实时绘图来实现。 接下来上自定义 button 的代码。 第一阶段 ~ 第二阶段 VoiceRecordButton.m 123456789101112131415161718192021222324252627282930- (void)startAnimation&#123; self.enabled = NO; //动画过程中禁用 [self moveAnchorPointToCenter]; //将锚点移到中心 (为了达到围绕中心缩放的效果) //放大 CAKeyframeAnimation *scaleToBigAnimation = [CAKeyframeAnimation animationWithKeyPath:@"transform.scale"]; scaleToBigAnimation.values = @[@(1.0), @(.7f), @(1.f), @(1.3f), @(1.7f)]; //先从1.0缩小到0.7，再放大到1.7，这样就实现了泡泡效果 scaleToBigAnimation.duration = 0.5; scaleToBigAnimation.beginTime = 0; //缩小 CAKeyframeAnimation *scaleAnimation = [CAKeyframeAnimation animationWithKeyPath:@"transform.scale"]; scaleAnimation.values = @[@(1.7f), @(1)]; scaleAnimation.duration = 0.75; scaleAnimation.beginTime = scaleToBigAnimation.beginTime + scaleToBigAnimation.duration; //位置下移，与缩小动画同时进行 CABasicAnimation *positionAnimation = [CABasicAnimation animationWithKeyPath:@"position.y"]; positionAnimation.toValue = @(self.layer.position.y - NSWidth(self.frame)); positionAnimation.duration = scaleAnimation.duration; positionAnimation.beginTime = scaleAnimation.beginTime; //添加动画组 CAAnimationGroup *animationGroup = [CAAnimationGroup animation]; animationGroup.delegate = self; animationGroup.duration = scaleToBigAnimation.duration + positionAnimation.duration; [animationGroup setValue:@"animationGroup" forKey:@"AnimationKey"]; animationGroup.animations = @[scaleToBigAnimation, scaleAnimation, positionAnimation]; [self.layer addAnimation:animationGroup forKey:@"animationGroup"];&#125; VoiceRecordButton.m123456789101112131415- (void)moveAnchorPointToCenter&#123; //由于图层锚点默认是在原点(0,0)，需要让图层围绕中心点缩放 self.layer.anchorPoint = CGPointMake(0.5, 0.5); //锚点改变后，为了让图层随着视图移动，将图层的位置也改到锚点的位置 NSRect rect = self.frame; CGFloat centerX = rect.origin.x + rect.size.width / 2.f; CGFloat centerY = rect.origin.y + rect.size.height / 2.f; self.layer.position = CGPointMake(centerX, centerY);&#125;- (void)resumeAnchorPoint&#123; self.layer.anchorPoint = CGPointZero; self.layer.position = self.frame.origin;&#125; 动画结束后，在代理里触发下一步操作。 VoiceRecordButton.m123456789101112#pragma mark - CAAnimation delegate- (void)animationDidStop:(CAAnimation *)anim finished:(BOOL)flag&#123; if (flag) &#123; if ([self.delegate respondsToSelector:@selector(voiceRecordingWillBegin)]) &#123; //执行代理方法，准备数据 [self.delegate voiceRecordingWillBegin]; &#125; [self resumeAnchorPoint]; [self startRecording]; &#125;&#125; 第三阶段 ~ 第四阶段 本文 demo 里所有的音频数据都是随机生成的模拟数据，也没有实现真正的录音哈。 控制器在 voiceRecordingWillBegin 这个方法里，准备需要绘制的数据。 ViewController.m1234567891011121314151617181920212223242526272829303132333435#pragma mark - voice recording button delegate//录音即将开始- (void)voiceRecordingWillBegin&#123; //之前的动画只是图层动画，图层已经到了目标位置，但视图的 frame 还在原来的位置，因此要修改视图的位置和尺寸 NSRect frame = self.voiceRecordBtn.frame; CGFloat centerX = frame.origin.x + frame.size.width / 2.f; CGFloat centerY = frame.origin.y + frame.size.height / 2.f - frame.size.height; frame.size.width = 400; //这是蓝色波形图的最大宽度 frame.origin.x = centerX - frame.size.width / 2.f; frame.origin.y = centerY - frame.size.height / 2.f; self.voiceRecordBtn.frame = frame; //添加计时器，构造模拟数据 [self addTimer];&#125;- (void)addTimer&#123; //添加定时器 _timer = [NSTimer scheduledTimerWithTimeInterval:.1f target:self selector:@selector(addPoint) userInfo:nil repeats:YES]; [[NSRunLoop mainRunLoop] addTimer:_timer forMode:NSRunLoopCommonModes];&#125;- (void)addPoint&#123; //随机点 NSPoint point = NSMakePoint(self.voiceRecordBtn.bounds.size.height / 2.f, arc4random_uniform(NSHeight(self.voiceRecordBtn.frame) / 4.f) + 0); //插入到数组（动画视图最右边），array添加CGPoint需要转换一下// [self.pointArray insertObject:[NSValue valueWithPoint:point] atIndex:0]; [self.pointArray addObject:[NSValue valueWithPoint:point]]; //传值，重绘视图 self.voiceRecordBtn.pointArray = self.pointArray;&#125; 回到自定义按钮的.m 文件 点数组的 setter 方法 VoiceRecordButton.m1234- (void)setPointArray:(NSArray *)pointArray&#123; _pointArray = pointArray; [self setNeedsDisplay:YES];&#125; 开始录音 VoiceRecordButton.m1234567- (void)startRecording&#123; self.talking = YES; [self setNeedsDisplay:YES]; //延迟_recordingDuration执行，若没有手动停止，则自动停止录音 [self performSelector:@selector(stopRecording) withObject:nil afterDelay:_recordingDuration];&#125; 停止录音 VoiceRecordButton.m1234567891011121314- (void)stopRecording&#123; //取消延迟执行 [NSObject cancelPreviousPerformRequestsWithTarget:self selector:@selector(stopRecording) object:nil]; self.enabled = YES; self.frame = self.initialFrame; //录音结束后，按钮回到点击前的初始状态 self.talking = NO; [self setNeedsDisplay:YES]; if ([self.delegate respondsToSelector:@selector(voiceRecordingDidFinish)]) &#123; [self.delegate voiceRecordingDidFinish]; &#125;&#125; 以下是重头戏了，每次调用[self setNeedsDisplay:YES]方法时，系统会自动调用drawRect:(NSRect)dirtyRect 方法，我们在这个方法里绘制数据。 VoiceRecordButton.m123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172- (void)drawRect:(NSRect)dirtyRect&#123; [super drawRect:dirtyRect]; if (!self.talking) &#123; //现在没有在录音，即初始状态 NSBezierPath *rectPath = [NSBezierPath bezierPathWithOvalInRect:dirtyRect]; [[NSColor blueColor] setFill]; [rectPath fill]; NSImage *image = [NSImage imageNamed:@"SideAudio"]; [image drawInRect:dirtyRect]; return; &#125; CGFloat midY = NSHeight(dirtyRect) / 2.f; CGFloat midX = NSWidth(dirtyRect) / 2.f; CGFloat leftX = midX - _pointArray.count / 2.f - _initialWidth / 2.f; CGFloat rightX = midX + _pointArray.count / 2.f + _initialWidth / 2.f; // Drawing code here. CGContextRef ctx = [[NSGraphicsContext currentContext] graphicsPort]; //绘制初始线型，模拟一般录音场景，刚开始可能没有说话，一条横线 CGMutablePathRef linePath = CGPathCreateMutable(); CGPathMoveToPoint(linePath, nil, leftX, midY); CGPathAddLineToPoint(linePath, nil, leftX + _initialWidth, midY); //_initialWidth 横线的宽度，这里给了个固定值 CGContextAddPath(ctx, linePath); //绘制上半部分波形 CGMutablePathRef halfPath = CGPathCreateMutable(); //绘制路径 CGPathMoveToPoint(halfPath, nil, NSWidth(dirtyRect), midY); for (NSInteger i = 0; i &lt; _pointArray.count; i++) &#123; NSValue *pointValue = _pointArray[i]; NSPoint point = pointValue.pointValue; NSInteger j = _pointArray.count - i - 1; if (point.y == 0) &#123; CGPathMoveToPoint(halfPath, nil, rightX - j + 1, midY); CGPathAddLineToPoint(halfPath, NULL, rightX - j, midY); &#125;else&#123; CGPathMoveToPoint(halfPath, nil, rightX - j, midY); CGPathAddLineToPoint(halfPath, NULL, rightX - j, midY + point.y); &#125; &#125; //实现波形图反转 CGMutablePathRef fullPath = CGPathCreateMutable();//创建新路径 CGPathAddPath(fullPath, NULL, halfPath); //合并路径 CGAffineTransform transform = CGAffineTransformIdentity; //反转 //反转配置 transform = CGAffineTransformTranslate(transform, 0, NSHeight(dirtyRect)); transform = CGAffineTransformScale(transform, 1.0, -1.0); CGPathAddPath(fullPath, &amp;transform, halfPath); //将路径添加到上下文中 CGContextAddPath(ctx, fullPath); //绘制矩形区域，即不断变长的蓝色背景 CGMutablePathRef rectPath = CGPathCreateMutable(); CGPathMoveToPoint(rectPath, nil, leftX, 0); CGPathAddRoundedRect(rectPath, nil, CGRectMake(leftX, 0, _pointArray.count + _initialWidth, NSHeight(dirtyRect)), NSHeight(dirtyRect) / 2.f, NSHeight(dirtyRect) / 2.f); CGContextAddPath(ctx, rectPath); CGContextSetLineWidth(ctx, 1); CGContextSetStrokeColorWithColor(ctx, [NSColor whiteColor].CGColor); CGContextSetFillColorWithColor(ctx, [NSColor blueColor].CGColor); CGContextDrawPath(ctx, kCGPathFillStroke); //移除 CGPathRelease(halfPath); CGPathRelease(fullPath);&#125; 结束，明天放播放录音的动画实现。 Demo 地址：https://github.com/YunFei2015/AudioWaveAnimation.git]]></content>
      <categories>
        <category>Mac开发</category>
      </categories>
      <tags>
        <tag>动画</tag>
        <tag>绘图</tag>
        <tag>Mac 开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于storyboard不得不说的事]]></title>
    <url>%2F2017%2F05%2F05%2Fstoryboard-loading%2F</url>
    <content type="text"><![CDATA[自 iOS 5 时代苹果推出 Storyboard（以下简称SB）以来，关于使用代码手写 UI、SB 和 xib 之间的争论就一直存在。问题主要在于 Storyboard 的加载过程是否会影响性能，下面就分析一下 Storyboard 中的 ViewController 是怎么加载出来的。 细心的同学会发现，在编译的时候 Xcode 有个 Compiling Storyboard files 的过程。编译过程中，项目里用到的 SB 文件也会被编译，并以 .storyboarc 为扩展名最终保存在 APP 包内。.storyboardc 是一个文件夹，里面保存了描述编译后的 SB 信息的 Info.plist 文件，以及一系列 .nib 文件。原来的 SB 中的每个对象（也就是每个 ViewController）将会被编译成一个单独的 .nib，.nib 中包含了编码后的对应的对象层级。在加载一个 SB，并从中读取 ViewController 类型和 nib 的关系，来完成 UIStoryboard 的初始化。接下来读取某个 nib，并使用UINibDecoder 进行解码，将 nib 二进制还原为实际的对象，最后调用该对象的 initWithCoder： 完成各个属性的解码。最后，调用 awakeFromNib 方法，来通知开发者从 nib 的加载已经完毕。 可以看出，从只有一个 ViewController 的 SB 中加载这个 VC，与从有多个 ViewController 的 SB 中加载的情况，速度上并不会有什么区别。 原文： 再看关于 Storyboard 的一些争论]]></content>
      <categories>
        <category>iOS开发</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>Interface Build</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AFNetworking README 中文版]]></title>
    <url>%2F2017%2F01%2F25%2FAFNetworking-translate%2F</url>
    <content type="text"><![CDATA[AFNetworking README 中文版AFNetworking是iOS和Mac OS X平台上的轻量级网络库，在Foudation URL加载系统基础上构建，对构建于Cocoa的强大高级的网络抽象进行了扩充。它具有设计良好的、富有特性的、模块化结构的API接口集。 体系结构NSURLSession 会话 AFURLSessionManager AFHTTPSessionManager 序列化 &lt;AFURLRequestSerialization&gt; AFHTTPRequestSerializer AFJSONRequestSerializer AFPropertyListRequestSerializer &lt;AFURLResponseSerialization&gt; AFHTTPResponseSerializer AFJSONResponseSerializer AFXMLParserResponseSerializer AFXMLDocumentResponseSerializer(Mac OS X) AFPropertyListResponseSerializer AFImageResponseSerializer AFCompoundResponseSerializer 其他功能 AFSecurityPolicy AFNetworkReachabilityManager 用法AFURLSessionManagerAFURLSessionManager在一个定义好的NSURLSessionConfiguration对象基础上创建并管理一个NSURLSession对象, NSURLSessionConfiguration遵循&lt;NSURLSessionTaskDelegate&gt;, &lt;NSURLSessionDataDelegate&gt;, &lt;NSURLSessionDownloadDelegate&gt;, &lt;NSURLSessionDelegate&gt;协议. 创建一个下载任务1234567891011NSURLSessionConfiguration *configuration = [NSURLSessionConfiguration defaultSessionConfiguration];AFURLSessionManager *manager = [[AFURLSessionManager alloc] initWithSessionConfiguration:configuration];NSURL *URL = [NSURL URLWithString:@"http://example.com/download.zip"];NSURLRequest *request = [NSURLRequest requestWithURL:URL];NSURLSessionDownloadTask *downloadTask = [manager downloadTaskWithRequest:request progress:nil destination:^NSURL *(NSURL *targetPath, NSURLResponse *response) &#123; NSURL *documentsDirectoryURL = [[NSFileManager defaultManager] URLForDirectory:NSDocumentDirectory inDomain:NSUserDomainMask appropriateForURL:nil create:NO error:nil]; return [documentsDirectoryURL URLByAppendingPathComponent:[response suggestedFilename]];&#125; completionHandler:^(NSURLResponse *response, NSURL *filePath, NSError *error) &#123; NSLog(@"File downloaded to: %@", filePath);&#125;];[downloadTask resume]; 创建一个上传任务12345678910111213NSURLSessionConfiguration *configuration = [NSURLSessionConfiguration defaultSessionConfiguration];AFURLSessionManager *manager = [[AFURLSessionManager alloc] initWithSessionConfiguration:configuration];NSURL *URL = [NSURL URLWithString:@"http://example.com/upload"];NSURLRequest *request = [NSURLRequest requestWithURL:URL];NSURL *filePath = [NSURL fileURLWithPath:@"file://path/to/image.png"];NSURLSessionUploadTask *uploadTask = [manager uploadTaskWithRequest:request fromFile:filePath progress:nil completionHandler:^(NSURLResponse *response, id responseObject, NSError *error) &#123; if (error) &#123; NSLog(@"Error: %@", error); &#125; else &#123; NSLog(@"Success: %@ %@", response, responseObject); &#125;&#125;];[uploadTask resume]; 创建一个多表单请求的上传任务，带进度条1234567891011121314151617181920212223NSMutableURLRequest *request = [[AFHTTPRequestSerializer serializer] multipartFormRequestWithMethod:@"POST" URLString:@"http://example.com/upload" parameters:nil constructingBodyWithBlock:^(id&lt;AFMultipartFormData&gt; formData) &#123; [formData appendPartWithFileURL:[NSURL fileURLWithPath:@"file://path/to/image.jpg"] name:@"file" fileName:@"filename.jpg" mimeType:@"image/jpeg" error:nil]; &#125; error:nil];AFURLSessionManager *manager = [[AFURLSessionManager alloc] initWithSessionConfiguration:[NSURLSessionConfiguration defaultSessionConfiguration]];NSURLSessionUploadTask *uploadTask;uploadTask = [manager uploadTaskWithStreamedRequest:request progress:^(NSProgress * _Nonnull uploadProgress) &#123; // This is not called back on the main queue. // You are responsible for dispatching to the main queue for UI updates dispatch_async(dispatch_get_main_queue(), ^&#123; //Update the progress view [progressView setProgress:uploadProgress.fractionCompleted]; &#125;); &#125; completionHandler:^(NSURLResponse * _Nonnull response, id _Nullable responseObject, NSError * _Nullable error) &#123; if (error) &#123; NSLog(@"Error: %@", error); &#125; else &#123; NSLog(@"%@ %@", response, responseObject); &#125; &#125;];[uploadTask resume]; 创建一个数据任务123456789101112NSURLSessionConfiguration *configuration = [NSURLSessionConfiguration defaultSessionConfiguration];AFURLSessionManager *manager = [[AFURLSessionManager alloc] initWithSessionConfiguration:configuration];NSURL *URL = [NSURL URLWithString:@"http://httpbin.org/get"];NSURLRequest *request = [NSURLRequest requestWithURL:URL];NSURLSessionDataTask *dataTask = [manager dataTaskWithRequest:request completionHandler:^(NSURLResponse *response, id responseObject, NSError *error) &#123; if (error) &#123; NSLog(@"Error: %@", error); &#125; else &#123; NSLog(@"%@ %@", response, responseObject); &#125;&#125;];[dataTask resume]; 请求序列化请求序列化器通过将参数编码为字符串或HTTP请求体，结合URL字符串创建一个请求。 字符串格式的参数编码1[[AFHTTPRequestSerializer serializer] requestWithMethod:@"GET" URLString:URLString parameters:parameters error:nil]; GET http://example.com?foo=bar&amp;baz[]=1&amp;baz[]=2&amp;baz[]=3 URL格式的参数编码1[[AFHTTPRequestSerializer serializer] requestWithMethod:@"POST" URLString:URLString parameters:parameters error:nil]; POST http://example.com/Content-Type: application/x-www-form-urlencodedfoo=bar&amp;baz[]=1&amp;baz[]=2&amp;baz[]=3 JSON格式的参数编码1[[AFJSONRequestSerializer serializer] requestWithMethod:@"POST" URLString:URLString parameters:parameters error:nil]; POST http://example.com/Content-Type: application/json{“foo”: “bar”, “baz”: [1,2,3]} 网络可达性管理AFNetworkReachabilityManager监控WWAN和WiFi的网络可达性。 不要使用Reachability决定是否发送原始请求； 应该尝试重发。 可以使用Reachability决定一个请求在什么时候自动重发； 虽然可能请求失败，但一个网络可达的Reachability通知是重新请求某些事物的好时候。 网络可达管理器是个很有用的工具，可以确定一个请求为什么失败。 一个网络请求失败后，通知用户“断线了”，比返回“请求超时”这种更技术更精确的错误要好。 参考 WWDC 2012 session 706, “Networking Best Practices.”. 共享的网络可达性管理器1234[[AFNetworkReachabilityManager sharedManager] setReachabilityStatusChangeBlock:^(AFNetworkReachabilityStatus status) &#123; NSLog(@"Reachability: %@", AFStringFromNetworkReachabilityStatus(status));&#125;];[[AFNetworkReachabilityManager sharedManager] startMonitoring]; 安全策略在你的APP里添加pinned X.509证书会帮你避免人为攻击和其他漏洞扫描。当应用处理敏感的用户数据或金融信息时，非常需要在具有SSL证书的HTTPS连接中执行所有操作。 ####允许无效SSL证书 12AFHTTPSessionManager *manager = [AFHTTPSessionManager manager];manager.securityPolicy.allowInvalidCertificates = YES; // not recommended for production]]></content>
      <categories>
        <category>第三方库</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Face++ 实现人脸识别]]></title>
    <url>%2F2016%2F11%2F30%2Fface-recognition-02%2F</url>
    <content type="text"><![CDATA[上次讲了如何使用Face++实现人脸搜索，这次我们来谈谈面部识别的实现方法。 效果图 将Face++集成到项目中详细步骤请参考使用 Face++实现人脸搜索第一部分。 开始工作1. 声明属性变量12@property (strong, nonatomic) UIImage personImg;//人物图像@property (weak, nonatomic) IBOutletUIImageView *imgView; 2. 从本地相册选取图片123UIImagePickerController *picker = [[UIImagePickerController alloc] init]; picker.sourceType = UIImagePickerControllerSourceTypePhotoLibrary; picker.delegate = self; [self presentViewController:picker animated:YES completion:nil]; 3. 选择图片后，进行面部识别123456789101112131415161718-(void)imagePickerController:(UIImagePickerController *)picker didFinishPickingMediaWithInfo:(NSDictionary&lt;NSString *,id&gt; *)info&#123; _personImg = info[UIImagePickerControllerOriginalImage]; [_imgView updateH:_imgView.width / (_personImg.size.width / _personImg.size.height)];//按原比例更新视图宽高，类别方法，此处就不赘述了 _imgView.image = _personImg; NSData *data = UIImageJPEGRepresentation(_personImg, 1); //进行人脸识别，由于耗费时间较长，最好异步执行 dispatch_async(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT, 0), ^&#123; FaceppResult *result = [[FaceppAPI detection] detectWithURL:nil orImageData:data mode:FaceppDetectionModeOneFace attribute:FaceppDetectionAttributeAll tag:nil async:NO others:nil]; //回到主线程更新UI dispatch_async(dispatch_get_main_queue(), ^&#123; [self didFinishDetection:result]; &#125;); &#125;); [picker dismissViewControllerAnimated:YES completion:^&#123; &#125;];&#125; result返回值说明，JSON示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&#123;"face": [ &#123;"attribute": &#123; "age": &#123;//包含年龄分析结果，value的值为一个非负整数表示估计的年龄, range表示估计年龄的正负区间 "range": 5, "value": 14 &#125;, "gender": &#123;//包含性别分析结果，value的值为Male/Female, confidence表示置信度 "confidence": 99.9981, "value": "Female" &#125;, "race": &#123;//包含人种分析结果，value的值为Asian/White/Black, confidence表示置信度 "confidence": 99.818, "value": "Asian" &#125;, "smiling": &#123;//包含微笑程度分析结果，value的值为0－100的实数，越大表示微笑程度越高 "value": 8.14031 &#125; &#125;, "face_id": "62bed8c94bbf92323675943328aed269", "position": &#123; "center": &#123;//检出的人脸框的中心点坐标, x &amp; y 坐标分别表示在图片中的宽度和高度的百分比 (0~100之间的实数) "x": 49.731183, "y": 32.5 &#125;, "eye_left": &#123;//相应人脸的左眼坐标，x &amp; y 坐标分别表示在图片中的宽度和高度的百分比 (0~100之间的实数) "x": 43.148441, "y": 28.722909 &#125;, "eye_right": &#123;//相应人脸的右眼坐标，x &amp; y 坐标分别表示在图片中的宽度和高度的百分比 (0~100之间的实数) "x": 55.413441, "y": 27.745455 &#125;, "height": 21.363636, //0~100之间的实数，表示检出的脸的高度在图片中百分比 "mouth_left": &#123;//相应人脸的左侧嘴角坐标，x &amp; y 坐标分别表示在图片中的宽度和高度的百分比 (0~100之间的实数) "x": 46.193978, "y": 39.472682 &#125;, "mouth_right": &#123;//相应人脸的右侧嘴角坐标，x &amp; y 坐标分别表示在图片中的宽度和高度的百分比 (0~100之间的实数) "x": 54.952688, "y": 38.989955 &#125;, "nose": &#123;//相应人脸的鼻尖坐标，x &amp; y 坐标分别表示在图片中的宽度和高度的百分比 (0~100之间的实数) "x": 50.23414, "y": 34.953727 &#125;, "width": 25.268817 //0~100之间的实数，表示检出的脸的宽度在图片中百分比 &#125;, "tag": "" &#125;], "img_height": 220, "img_id": "064aae142ef977bb44b6cc3dd9a6eeb1", "img_width": 186, "session_id": "ade961e7b0d347ca943e0f0f752f7cc5", "url": null&#125; 4. 对检测结果进行处理123456- (void)didFinishDetection:(FaceppResult *)result&#123; NSDictionary *face = [result content][@"face"][0]; [self markTheFiveSenseOrgansWithData:face]; [self markFaceWithData:face]; [self analysisPersonWithData:face];&#125; 1) 标记脸部范围 1234567891011121314151617- (void)markFaceWithData:(NSDictionary *)face&#123; CGFloat img_width = _imgView.frame.size.width; CGFloat img_height = _imgView.frame.size.height; CGFloat center_x = [face[@"position"][@"center"][@"x"] floatValue] / 100.f * img_width;//脸部中心点x坐标 CGFloat center_y = [face[@"position"][@"center"][@"y"] floatValue] / 100.f * img_height;//脸部中心点y坐标 CGFloat width = [face[@"position"][@"width"] floatValue] / 100.f * img_width;//脸部宽度 CGFloat height = [face[@"position"][@"height"] floatValue] / 100.f * img_height;//脸部高度 CGPoint center = CGPointMake(center_x, center_y); CGRect rect = CGRectMake(0, 0, width, height); //自定义一个CALayer,画一个矩形 RectLayer *rectLayer = [RectLayer layer]; rectLayer.bounds = rect; rectLayer.position = center; [rectLayer setNeedsDisplayInRect:rect]; [self.imgView.layer addSublayer:rectLayer]; //自定义一个CALayer,画一个矩形 [rectLayer twinkleAnimation:3];&#125; 2) 标记五官 12345678910111213141516171819202122232425262728293031323334353637383940414243- (void)markTheFiveSenseOrgansWithData:(NSDictionary *)face&#123; CGFloat img_width = _imgView.frame.size.width; CGFloat img_height = _imgView.frame.size.height; CGFloat eye_left_x = [face[@"position"][@"eye_left"][@"x"] floatValue] / 100.f * img_width; CGFloat eye_left_y = [face[@"position"][@"eye_left"][@"y"] floatValue] / 100.f * img_height; CGFloat eye_right_x = [face[@"position"][@"eye_right"][@"x"] floatValue] / 100.f * img_width; CGFloat eye_right_y = [face[@"position"][@"eye_right"][@"y"] floatValue] / 100.f * img_height; CGFloat mouth_left_x = [face[@"position"][@"mouth_left"][@"x"] floatValue] / 100.f * img_width; CGFloat mouth_left_y = [face[@"position"][@"mouth_left"][@"y"] floatValue] / 100.f * img_height; CGFloat mouth_right_x = [face[@"position"][@"mouth_right"][@"x"] floatValue] / 100.f * img_width; CGFloat mouth_right_y = [face[@"position"][@"mouth_right"][@"y"] floatValue] / 100.f * img_height; CGFloat nose_x = [face[@"position"][@"nose"][@"x"] floatValue] / 100.f * img_width; CGFloat nose_y = [face[@"position"][@"nose"][@"y"] floatValue] / 100.f * img_height; //标注左眼 CALayer *leftEyeLayer = [CALayer layer]; leftEyeLayer.frame = CGRectMake(eye_left_x, eye_left_y, 2, 2); leftEyeLayer.backgroundColor = [UIColor greenColor].CGColor; [leftEyeLayer setNeedsDisplay]; [self.imgView.layer addSublayer:leftEyeLayer]; //标注右眼 CALayer *rightEyeLayer = [CALayer layer]; rightEyeLayer.frame = CGRectMake(eye_right_x, eye_right_y, 2, 2); rightEyeLayer.backgroundColor = [UIColor greenColor].CGColor; [rightEyeLayer setNeedsDisplay]; [self.imgView.layer addSublayer:rightEyeLayer]; [leftEyeLayer twinkleAnimation:3]; [rightEyeLayer twinkleAnimation:3]; //标注鼻子 RoundLayer *noseLayer = [RoundLayer layer]; noseLayer.bounds = CGRectMake(0, 0, 20, 20); noseLayer.position = CGPointMake(nose_x, nose_y); [noseLayer setNeedsDisplay]; [self.imgView.layer addSublayer:noseLayer]; [noseLayer twinkleAnimation:3]; //标注嘴巴 RectLayer *mouthLayer = [RectLayer layer]; mouthLayer.frame = CGRectMake(mouth_left_x, mouth_left_y, mouth_right_x - mouth_left_x, mouth_right_y - mouth_left_y); [mouthLayer setNeedsDisplay]; [self.imgView.layer addSublayer:mouthLayer]; [mouthLayer twinkleAnimation:3];&#125; 3) 分析人物特征 123456789101112131415161718192021- (void)analysisPersonWithData:(NSDictionary *)face&#123; CGFloat smiling = [face[@"attribute"][@"smiling"][@"value"] floatValue];//笑容 NSInteger age_value = [face[@"attribute"][@"age"][@"value"] integerValue];//年龄 NSInteger age_range = [face[@"attribute"][@"age"][@"range"] integerValue];//年龄误差 NSString *gender = face[@"attribute"][@"gender"][@"value"];//性别 NSString *race = face[@"attribute"][@"race"][@"value"];//种族 NSArray *texts = @[[NSString stringWithFormat:@"开心度：%@", @(smiling).stringValue], [NSString stringWithFormat:@"年龄：%ld - %ld",age_value - age_range, age_value + age_range], [NSString stringWithFormat:@"性别：%@", gender], [NSString stringWithFormat:@"种族：%@", race]]; CGFloat width = self.view.width - 40; CGFloat height = 20; CGFloat x = 20; CGFloat margin = 10; for (int i = 0; i &lt; 4; i++) &#123; CGFloat y = _imgView.height + _imgView.y + height * i + margin * (i + 1); UILabel *label = [[UILabel alloc] initWithFrame:CGRectMake(x, y, width, height)]; label.text = texts[i]; [self.view addSubview:label]; &#125;&#125; 5. 自定义layer1）RectLayer重写drawInContext方法 12345678910-(void)drawInContext:(CGContextRef)ctx&#123; //1.绘制图形 //画一个矩形 CGContextAddRect(ctx, self.bounds); //设置属性（颜色） CGContextSetLineWidth(ctx, 2); CGContextSetRGBStrokeColor(ctx, 0, 1, 0, 1); //2.渲染 CGContextStrokePath(ctx);&#125; 2）RoundLayer重写drawInContext方法 123456-(void)drawInContext:(CGContextRef)ctx&#123; CGContextAddEllipseInRect(ctx, self.bounds); CGContextSetLineWidth(ctx, 2); CGContextSetRGBStrokeColor(ctx, 0, 1, 0, 1); CGContextStrokePath(ctx);&#125; 6. 创建CALayer类别，管理layer动画12345678910111213141516#import "CALayer+Animation.h"@implementation CALayer (Animation)- (void)twinkleAnimation:(float)time&#123; CABasicAnimation *animation = [CABasicAnimation animationWithKeyPath:@"opacity"]; animation.fromValue = @[@1.0f]; animation.toValue = @[@0.0f]; animation.autoreverses = YES; animation.duration = time * 0.1; animation.repeatCount = time - 1; animation.removedOnCompletion = NO; animation.fillMode = kCAFillModeForwards; [self addAnimation:animation forKey:nil];&#125;@end]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Face++ 实现人脸搜索]]></title>
    <url>%2F2016%2F11%2F22%2Fface-recognition-01%2F</url>
    <content type="text"><![CDATA[Face++由三位85后的清华师兄弟于2011年创建，先后获得联想之星的天使投资和创新工场的A轮投资。2013年底开始，先后在世界最权威的人脸检测(FDDB评测)、人脸关键点定位（300-W评测），和人脸识别(LFW评测)获得三个世界第一。这意味着在人脸技术三个最核心的技术模块，Face++都达到了世界最高水平。同时Face++所提供的技术云平台(Face++ 最好的免费人脸识别云服务)也是世界上最大的人脸技术平台，累计处理图片总数接近10亿规模，并为阿里，联想，360，世纪佳缘，美图秀秀，camera360等一批国内外著名互联网企业提供了技术服务。 在今年6月份的WWDC主题演讲大会上，苹果介绍了一项全新的人脸识别功能。这项功能被添加在 iOS 10 的照片应用当中，会根据面部表情对照片进行分类。苹果软件工程高级副总裁克雷格·费德里奇在演讲中表示，“先进计算机视觉”（Advanced Computer Vision）是今年照片应用最大幅度的一次更新，因为苹果通过深度学习技术将人脸识别带到了 iPhone 身上。 本文将介绍如何通过Face++来实现人脸搜索。 准备工作1. 请到GitHub上下载SDK：facepp-ios-sdk 2. 解压后可以看到两个目录：FaceppSDK和FaceppSDK_ARC。顾名思义，FaceppSDK_ARC是在ARC环境下使用的，FaceppSDK是在MRC环境下使用的，本文着重介绍ARC环境的实现方法。 3. 在Finder中，将FaceppSDK_ARC目录拖入工程目录下，将FaceppSDK_ARC添加至您的工程中; 4. 在应用程序入口处添加以下代码： 1#import "FaceppAPI.h"` 在入口代码中添加如下代码: 12345#import "FaceppAPI.h"- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions &#123; [FaceppAPI initWithApiKey: @"YOUR_KEY" andApiSecret: @"YOUR_SECRET"]; ...&#125; 5. 使用调试模式： 开启调试模式之后，程序将向控制台输出所有http请求的url，以及返回json的原始数据，以供调试使用。 1[FaceppAPI setDebugMode: TRUE]; ###开始工作 1. 使用/detection/detect接口来对图片进行检测，在获得数据以后其将返回一个FaceppResult结构 12FaceppResult *result = [[FaceppAPI detection] detectWithURL:nil imageData:[NSData dataWithContentsOfFile:@"LOCAL_FILE_PATH"]]; 2. 从FaceppResult中获取结果 获得第一张脸的face_id： 1NSString *face_id = [result content][@"face"][0][@"face_id"]; 3. 对所有图片执行1~2步后，把每个图片对应的face_id存入数组，然后使用/faceset/create接口来创建faceset 12FaceppResult *setResult = [[FaceppAPI faceset] createWithFacesetName:nil andFaceId:_facesIds andTag:@"1"];NSString *faceset_id = [setResult content][@"faceset_id"]; 4. 针对search功能对一个faceset进行训练 1[[FaceppAPI train] trainAsynchronouslyWithId:_faceset_id orName:nil andType:FaceppTrainSearch]; 在一个faceset内进行search之前，必须先对该faceset进行Train。 5. train成功后，针对某张图片的face_id进行搜索 1FaceppResult *searchResult = [[FaceppAPI recognition] searchWithKeyFaceId:face_id andFacesetId:_faceset_id orFacesetName:nil]; 搜索到的相似图片的face_id都存储在[searchResult content][@&quot;candidate&quot;]里，以下为运行结果： 123456789101112131415161718192021 [FacePlusPlus]response JSON: &#123; "candidate": [ &#123; "face_id": "589bb6f9c7cd98a45c662a93f4607316", "similarity": 99.9909, "tag": "" &#125;, &#123; "face_id": "4aa39ac1c989ff1e597dafa8979446be", "similarity": 75.3242, "tag": "" &#125;, &#123; "face_id": "6ae11866cf062abb1908087b7467a03a", "similarity": 59.9056, "tag": "" &#125; ], "session_id": "e37308bb0e354c75b87fdf1de9b7e5c7"&#125;]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vImage学习笔记——卷积（Convolution）]]></title>
    <url>%2F2016%2F08%2F31%2FvImage-translation-02%2F</url>
    <content type="text"><![CDATA[卷积（Convolution）是一个常用的图像处理技术，可以改变像素强度，从而影响周围其他像素的强度。卷积的常用技术是创建滤镜，使用卷积技术，你可以获取一些流行的图像效果，比如模糊（blur）、锐化（sharpen）及边缘检测（edge detection），这些效果在Photo Booth、iPhoto和Aperture都有广泛使用。 如果你对图像滤镜和实时处理有兴趣的话，你会发现vImage函数集的好处。用图像滤镜，卷积操作可以完成一些常用的滤镜效果，比如浮雕、模糊及色调分离。 vImage卷积技术对锐化或增强图像质量也很有用。当处理一些科学图像时，增强图像质量很有用。此外，由于科学图像通常都很大，就很有必要使用这些vImage技术来达到合适的性能需求。这种情况下你需要用到的技术有边缘检测（edge detection）、锐化、描绘外观轮廓（surface contour outlining）、平滑、及动作检测（motion detection）。 本章节讲述了卷积技术，以及如何使用vImage提供的卷积函数。通过本文，你可以： 了解卷积技术可以实现哪些效果； 学习什么是卷积核以及如何构建卷积核； 通过代码示例，学习如何对一个图像使用卷积技术。 卷积核（Convolution Kernels）图1展示了一个图像通过vImage卷积函数添加了浮雕效果前后的对比图。为了达到这个效果，vImage使用一个类网格的数学概念，称为核（kernel），来完成卷积操作。 图1 浮雕 图2是一个3×3的kernel。kernel的高度和宽度不必一样，但必须是奇数。kernel内部的数值会影响卷积的整体效果。这些数值决定了初始图像像素会如何转换成目标图像像素，这看起来可能不是很直观，9个数字会如何影响到滤镜效果呢？卷积技术经过一系列的操作，根据周围像素的强度改变当前像素的强度。vImage根据kernel执行卷积操作，这种通过kernel执行卷积计算的过程就称为kernel convolution（核卷积）。 图2 3×3卷积 卷积是像素单位的操作，即对每个像素都要执行同样的算法。因此，大图像比小图像需要更多的卷积操作。一个kernel可以被看做一个二维的网格数据，而图像也可以被看做一个二维网格数据（如图3），对一个图像应用kernel可以想象成把一个小格子（kernel）平铺在大格子（图像）上。 图3 图像是二维网格数据 kernel内部的数值会作为与它下面的数值相乘的乘数，下面的数值指的是被kernel数值覆盖的像素的强度。在进行卷积计算时，把kernel的中心值覆盖在待转换的像素上，然后将kernel的每个值与其正下方的像素值相乘，最后将所有的结果相加，相加后的结果就是新的像素强度。图4展示了kernel是如何转化像素的。 图4 核卷积 虽然kernel会覆盖到一些其它的像素，但是最终只有kernel中心值正下方的原始像素会发生变化。kernel和图像之间所有乘积相加后的和被称为加权和。为了确保处理后的图像对比原图不会过于饱和，vImage有一个常用的方法，就是设置一个除数因子，把加权和进行拆分。因为用周围像素的加权和来代替原始像素时常导致像素强度过大（并且图像整体也过于明亮），拆分加权和可以按比例降低滤镜效果的强度，并确保维持原始亮度，这个过程称为标准化。这个行为是可选的，被拆分后的加权和会代替原始像素值。kernel对每个像素重复这个过程。 注：如果要执行标准化，你必须向卷积函数提供你要使用的因子。因子最好是2的幂次方。你也可以在图像像素值为整数的时候再提供因子。浮点型不需要使用，因为你可以直接依比例决定kernel的浮点型数值来达到标准化。 kernel的数据类型和图像的数据类型必须保持一致，比如，如果图像像素数据类型是浮点型，那么kernel中的数据类型也必须是浮点型。 记住以上所述的算法vImage都已经帮你做好了，你不需要牢记卷积算法的步骤。当然，你也可以在自定义的核中实现该算法。 反卷积反卷积指的是解除先前的卷积效果——一般是原始图像中物理携带的卷积效果，比如镜头中的衍射效果。通常，反卷积是一个锐化操作。 反卷积的算法比较多，vImage用的是Richardson-Lucy deconvolution算法。 Richardson-Lucy deconvolution算法的目标是根据卷积后的像素值找到原始的像素值，以及kernel数据。 基于以上需求，在使用反卷积函数时必须提供卷积后的图像及卷积使用的kernel值。 vImage会自行处理反卷积的每一步操作，因此不需要牢记这些步骤。使用反卷积的时候，必须提供初始的卷积kernel（如果该kernel不对称的话，还要额外提供一个对角线翻转的kernel2）。 使用卷积核现在你最好了解一下核的结构以及卷积的处理过程，是时候使用几个vImage函数来看看了。本章节展示了如何实现图1中的浮雕效果，并解释了无偏差卷积和带偏差卷积之间的差别。 卷积vImage可以自动进行卷积计算，而你的工作是提供kernel，即描述卷积应该生成什么效果。表1展示了如何使用卷积去生成浮雕效果。你也可以通过合适的kernel，利用同样的代码来生成一个不同的效果，比如锐化。 表1 生成浮雕效果 123456789101112131415161718192021222324252627282930313233343536int myEmboss(void *inData, unsigned int inRowBytes, void *outData, unsigned int outRowBytes, unsigned int height, unsigned int width, void *kernel, unsigned int kernel_height, unsigned int kernel_width, int divisor , vImage_Flags flags )&#123; uint_8 kernel = &#123;-2, -2, 0, -2, 6, 0, 0, 0, 0&#125;; // 1 vImage_Buffer src = &#123; inData, height, width, inRowBytes &#125;; // 2 vImage_Buffer dest = &#123; outData, height, width, outRowBytes &#125;; // 3 unsigned char bgColor[4] = &#123; 0, 0, 0, 0 &#125;; // 4 vImage_Error err; // 5 err = vImageConvolve_ARGB8888( &amp;src, //const vImage_Buffer *src &amp;dest, //const vImage_Buffer *dest, NULL, 0, //unsigned int srcOffsetToROI_X, 0, //unsigned int srcOffsetToROI_Y, kernel, //const signed int *kernel, kernel_height, //unsigned int kernel_width, //unsigned int divisor, //int bgColor, flags | kvImageBackgroundColorFill //vImage_Flags flags ); return err;&#125; 上述代码都做了哪些工作呢？ 声明一个浮雕kernel，即int型数组。kernel的数据类型要与相应的vImage函数所需数据类型相匹配。示例中使用了vImageConvolve_ARGB8888函数，因此kernel的数据类型应该是uint_8（无符号，8-bit，整数）。kernel的数组元素从左至右，依次是第一行、第二行、第三行； 声明一个vImage_Buffer变量，用来存储原始图像信息。图像数据以数组的形式进行存储，元素图像二进制数据(inData)，另外还存储高度、宽度和每行的字节数。这样vImage会知道要处理的图像大小，并如何合适地处理它。 声明一个vImage_Buffer变量，用来存储目标图像信息。 声明一个Pixel8888-格式的像素来表示目标图像的背景色（示例中用的是黑色）。 声明一个vImage_Err变量来存储卷积函数的返回值。 然后，将这些声明的变量值传入vImageConvolve_ARGB8888函数中，由vImage来处理后续的计算，并把结果存储到dest变量中。vImageConvolve_ARGB8888函数是vImage中仅有的几个卷积函数之一。通常，vImage会为每种图像格式提供4种函数变体，ARGB8888前缀表示该函数处理的是交叉型图像（full-color），每个像素由四个8字节的整数构成一组，代表alpha（A），red（R），green（G）和blue（B）四个通道。想了解vImage支持的图像格式的更多细节，请看vImage概述。 示例myEmboss中还使用了vImage_Flags参数。该参数由1个或多个flags（用或逻辑运算符 | 连接）组成。kvImageBackgroundColorFill表示vImage要使用预先提供的背景色。 为了熟悉kernel效果的使用方法，请在你自己的代码中使用以下两个kernel。图6的kernel可以生成高斯模糊效果，图7的kernel可以生成边缘检测效果。 图6 高斯模糊 图7 边缘检测 带偏差的卷积在执行卷积操作时，可以选择是否带偏差。偏差是指在卷积结果上再额外添加一个来自周围像素的影响。由于某些卷积计算得到的结果可能为负值，偏差可以避免信号溢出。可以把偏差设为127或128，来允许负值也被描绘出来。偏差可能使整体图像效果变亮或变暗。 每个标准的vImage函数（比如vImageConvolve_PlanarF）都有一个对应的带偏差的函数（vImageConvolveWithBias_PlanarF）。偏差函数的使用方法和无偏差函数一样，除了必须设置bias参数来使用偏差。bias的数据类型必须与图像像素数据类型一致。 图8 带偏差与无偏差 使用高速滤镜vImage提供了一些特定的卷积函数，这些比一般的卷积函数具有更快的处理速度。OS X v10.4以上的系统，对于Planar_8和ARGB8888数据类型可以使用box滤镜和tent滤镜。这些滤镜可以得到模糊效果，函数是根据他们在笛卡尔坐标系的形状命名的。调用这些函数不需要提供kernel，效果等同于需要提供kernel的一般的卷积函数。但是这些函数比一般的函数性能上要快大约1个量级。 注：由于这些函数需要一个稳健精确的算法，vImage规定这些函数不支持浮点型。浮点型的计算误差会导致图像高密度区域附近的低密度区域显得人工化或粗糙化。 box滤镜用周围像素的未加权的平均数来代替被处理的像素值，相当于通过所有值都为1的kernel来进行卷积处理。对应的函数是vImageBoxConvolve_Planar8和vImageBoxConvolve_ARGB8888。每个转换后的像素都是其周围像素的平均值（周围像素的宽、高即kernel的宽、高）。 图9 box滤镜 tent滤镜用周围像素的加权平均值来代替被处理的像素值。对应的函数是vImageTentConvolve_Planar8和vImageTentConvolve_ARGB8888。tent滤镜的模糊操作相当于使用值不为1的kernel进行的卷积操作。和vImageBoxConvolve_Planar8和vImageBoxConvolve_ARGB8888一样，不需要向函数提供kernel值，只需要宽高即可。 图10 tent滤镜 假设kernel的大小是3×5。那么第一个矩阵是 第二个矩阵是 那么生成的kernel是 3×5的tent滤镜操作相当于使用上图中的滤镜来进行卷积操作。 使用多核vImage允许在单个卷积操作中使用多个kernel。可以使用vImageConvolveMultiKernel函数，分别定义四个kernel，每个kernel对应一个图像通道。一个kernel控制一个通道的话，你就可以对图像进行更高级别的处理。例如，你可以利用多核卷积对图像的颜色通道分别重新采样，抵消屏幕上的RGB荧光效果。由于四个kernel可以分别对单个颜色通道进行处理，vImageConvolveMultiKernel函数只能应用于交叉型图像。 这些函数的使用方法与单核卷积函数使用方法相同，唯一不同的是，需要提供一个指针数组，数组中每个元素指向一个kernel地址。 反卷积与卷积计算相同的是，vImage同样在内部封装了反卷积的计算过程，你只需要提供一个kernel即可。表2展示了如何把浮雕效果通过反卷积消除的过程。你可以用同样的代码、合适的kernel去反卷积各种效果（比如模糊效果）。但是不同于卷积操作的是，反卷积函数还需要另一个kernel参数。除非kernel的宽和高相同，否则这个参数不能为NULL。如果kernel的宽和高不相等，必须再提供一个行列反转的kernel。 表2是一段示例代码，描述了如何使用vImage对一个ARGB8888-格式的图像进行反卷积。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647int myDeconvolve(void *inData, unsigned int inRowBytes, void *outData, unsigned int outRowBytes, unsigned int height, unsigned int width, void *kernel, unsigned int kernel_height, unsigned int kernel_width, int divisor, int iterationCount, vImage_Flags flags )&#123; //Prepare data structures uint_8 kernel = &#123;-2, -2, 0, -2, 6, 0, 0, 0, 0&#125;; // 1 vImage_Error err; // 2 unsigned char bgColor[4] = &#123; 0, 0, 0, 0 &#125;; // 3 vImage_Buffer src = &#123; inData, height, width, inRowBytes &#125;; // 4 vImage_Buffer dest = &#123; outData, height, width, outRowBytes &#125;; // 5 //Send data to vImage for processing err = vImageRichardsonLucyDeConvolve_ARGB8888( &amp;src, // 6 &amp;dest, //const vImage_Buffer *dest, NULL, 0, //unsigned int srcOffsetToROI_X, 0, //unsigned int srcOffsetToROI_Y, kernel, //const signed int *kernel, NULL, //assumes symmetric kernel kernel_height, //unsigned int kernel_height, kernel_width, //unsigned int kernel_width, 0, //height of second kernel 0, //width of second kernel divisor, //int 0, //for second kernel bgColor, iterationCount, //uint32_t kvImageBackgroundColorFill | flags //vImage_Flags ); //Report result return err; // 7&#125; 这段代码都做了什么？ 定义vImage要进行反卷积处理的初始卷积图像的kernel值。示例使用的是对称的kernel（宽高都为3），因此不用再定义第二个kernel； 声明一个vImage_Error结构体，来存储反卷积结果； 声明一个Pixel8888-类型的像素，用于表示转换后图像的背景色； 声明一个vImage_Buffer结构体，用于存储初始图像信息。图像数据是作为一个字节型数组存储的，包括图像数据inData、宽、高、行字节等信息。这些信息可以让vImage知道它要处理的图像有多大，从而更好的执行操作； 声明一个vImage_Buffer结构体，用于存储目标图像信息； 把上述声明的变量传给vImage函数。注意示例中在调用vImageRichardsonDeConvolve_ARGB8888函数时，第二个kernel为NULL，这是因为第一个kernel是对称的，没有必要再进行翻转。 反卷积是一个迭代过程。你可以设置迭代次数，次数不同，得到的结果会不一样，因此，为了得到最理想的结果，可以多试几次。]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vImage学习笔记（一）——概述]]></title>
    <url>%2F2016%2F08%2F22%2FvImage-translation-01%2F</url>
    <content type="text"><![CDATA[一、关于图像格式 Image Formats图像格式（Image Formats）规定了像素数据如何在内存中存储。图像文件格式（比如JPG、PNG、GIF等）用来在程序中转换图像数据，并将数据存储在硬盘上。诸如Image I/O等框架可以从硬盘加载各种格式的图像文件，并且在内存中使用。在内存中，图像是通过二维数组来存储像素数据的，图像中的每个像素都对应数组中的一个元素。 图像格式（Image Formats）包含两种类型：二维平面型（planar）和交叉型（interleaved）。二维平面型图像把不同通道的数据存储在不同的缓冲区内，一个典型的平面型图像通常包括red、green、blue和alpha四个通道。交叉型图像把不同通道的数据轮换着存储：ARGBARGBARGB…… 图像数据可以是整型（integer）和浮点型（float）。在vImage中，通过一个8位（bit）的无符号整型数值表示色饱和度等级。数值范围0~255，255表示最大色饱和度，0表示无色饱和度。浮点型数值通常从0.0~1.0表示色饱和度。 以下是核心操作使用到的图像格式： Planar8 单通道（颜色或alpha）图像。每个像素都是一个8bit的无符号整数，数据类型是Pixel_8 PlanarF 单通道（颜色）图像。每个像素都是一个32bit的浮点数，数据类型是Pixel_F ARGB8888 图像包含四个交叉通道，alpha、red、green、blue，顺序固定。每个像素都是一个32位的数组（包含四个8位无符号整数）。数据类型是Pixel_8888 ARGBFFFF 图像包含四个交叉通道，alpha、red、green、blue，顺序固定。每个像素都是一个包含四个浮点数的数组。数据类型是Pixel_FFFF RGBA8888 图像包含四个交叉通道，red、green、blue、alpha，顺序固定。每个像素都是一个32位的数组（包含四个8位无符号整数）。数据类型是Pixel_8888 RGBAFFFF 图像包含四个交叉通道，red、green、blue、alpha，顺序固定。每个像素都是一个包含四个浮点数的数组。数据类型是Pixel_FFFF 可以将其他格式的图像转换为vImage的图像格式。例如，可以通过vImageConvert_16SToF函数将一个16位像素的图像转换为vImage支持的32位像素。下面这些函数可以帮助你在vImage图像格式之间进行转换，也可以把vImage不支持的图像格式转换为vImage格式。 vImageConvert_16SToF 将一个16位符号整型planar图像格式（或 vImage_Buffer.width=4的interleaved-multiply）缓冲区（ vImage_Buffer ）转换为浮点型数值缓冲区 vImageConvert_16UToF 将一个16位无符号整型planar图像格式（或 vImage_Buffer.width=4的interleaved-multiply）缓冲区（ vImage_Buffer ）转换为浮点型数值缓冲区 ImageConvert_FTo16S 将一个浮点型planar图像格式（或 vImage_Buffer.width=4的interleaved-multiply）缓冲区（ vImage_Buffer ）转换为16位符号整型缓冲区 vImageConvert_FTo16U 将一个浮点型planar图像格式（或 vImage_Buffer.width=4的interleaved-multiply）缓冲区（ vImage_Buffer ）转换为16位无符号整型缓冲区 vImageConvert_16UtoPlanar8 将一个16位无符号整型planar图像格式（或 vImage_Buffer.width=4的interleaved-multiply）缓冲区（ vImage_Buffer ）转换为8位整型缓冲区 vImageConvert_Planar8to16U 将一个8位整型planar图像格式（或 vImage_Buffer.width=4的interleaved-multiply）缓冲区（ vImage_Buffer ）转换为16位无符号整型缓冲区 vImageConvert_ARGB1555toPlanar8 将16位/像素图像（alpha通道1bit，red/green/blue通道5bit）转换为Planar8格式。 vImageConvert_ARGB1555toARGB8888 将16位/像素图像（alpha通道1bit，red/green/blue通道5bit）转换为ARGB8888格式。 vImageConvert_Planar8toARGB1555 将Planar8格式图像转换为包含1bit alpha，5bit red，5bit green，5bit blue的16位/像素图像。 vImageConvert_ARGB8888toARGB1555 将ARGB8888格式图像转换为包含1bit alpha，5bit red，5bit green，5bit blue的16位/像素图像。 vImageConvert_RGB565toPlanar8 将5bit red，6bit green，5bit blue的16位/像素图像转换为Planar8 vImageConvert_RGB565toARGB8888 将5bit red，6bit green，5bit blue的16位/像素图像转换为ARGB8888 vImageConvert_Planar8toRGB565 将Planar8图像转换为5-6-6图像 vImageConvert_ARGB8888toRGB565 将ARGB8888图像转换为5-6-5图像 vImageConvert_Planar16FtoPlanarF 将16位浮点型planar图像转换为32位浮点型 二、vImage练习Loading Image Data要将vImage集成到你的应用中，首先要把raw图像数据加载到内存。可以使用Image I/O框架把任何主流的图像文件（JPG、PNG、GIF等）加载到C类型缓冲池中（void *arrays）。 以下是从本地文件提取raw图像数据的例子： 12345678910111213141516171819202122NSURL* url = [NSURL fileURLWithPath:filename];//Create the image source with the options left null for now//Keep in mind since we created it, we're responsible for getting rid of itCGImageSourceRef image_source = CGImageSourceCreateWithURL( (CFURLRef)url, NULL);if(image_source == NULL)&#123; //Something went wrong fprintf(stderr, "VImage error: Couldn't create image source from URL\n"); return false;&#125;//Now that we got the source, let's create an image from the first image in the CGImageSourceCGImageRef image = CGImageSourceCreateImageAtIndex(image_source, 0, NULL); //We created our image, and that's all we needed the source for, so let's release itCFRelease(image_source); if(image == NULL)&#123; //something went wrong fprintf(stderr, "VImage error: Couldn't create image source from URL\n"); return false;&#125; 把图像加载到内存之后，就可以通过vImage函数进行各种处理了。请密切注意函数下划线之后的字符，那代表了像素数据的格式。vImage函数既可以在源缓冲区中直接处理，也可以在提供的目标缓冲区中处理。 由于vImage只负责处理图像，你还需要想办法把图像显示出来。根据你的应用开发环境（Carbon or Cocoa），你需要找到一个方法（例如Quartz）去显示合成后的像素数据，或将图像数据存储到磁盘（Image I/O）。 使用二维图像格式大多数vImage函数是从四种图像格式开始的。二维图像每次编码一个通道（先存储所有的红色通道数据，然后是绿色，蓝色，alpha），而交叉图像在内存是混合存储所有通道的。 注意：有时候你可能并不需要处理所有的通道。比如，你知道你要处理的图像中是不需要alpha通道的，或可能你的图像是灰度图，因此你只需要一个通道。这种情况下，使用二维图像格式可以让你把需要的通道隔离出来。 拼贴（Tiles）技术在图像应用中，通常要使用Tiles将一个图像分割成几个小图。之所以称为tiling技术，是因为这看起来很像是把多个地板瓷砖拼接成一大块儿的过程，图像中多个小的单位拼贴在一起，形成一个大的图像。这个技术的优势在于，把数据分散成N个小的单位后，可以分散填充到高速缓存中，这使CPU的处理速度加快了许多。 通常来说，当被处理的数据（包括输入数据和输出数据）放在处理器的数据缓存中时，vImage具有更好的性能。访问处理器缓存中的数据比访问内存数据要快很多。然而CPU缓存是很快，但是它在空间上是有限制的。不同的CPU缓存大小不一样，但总的来说，在Intel处理器中保存少于2MB的tile图像，还有在PowerPC处理器中保存少于512KB的tile还是很有优势的。 以下是Tiles技术的一些技巧： 有些CPU缓存一次只能保存很少量的数据（通常是512KB或更少） 128KB - 512KB的tile数据吞吐量最优 很多vImage函数内部可以使用tile技术（还有多线程）。如果你想自己控制tiling，可以在调用函数时在flags参数中添加kvImageDoNotTile标记，这样可以避免函数在内部使用tiling或多线程技术。 对于平方形tile，128KB-256KB的tile大小具有最好的吞吐率。 不同函数的最优tile大小也是不同的。具有少量字节计算的函数（大多数是转换函数）在file size 小于16KB时是最快的，典型的vImage函数在256KB tile size下是最快的。 数据缓冲排列当分配图像的浮点型数据时，保持4个字节的排列是很重要的。这说明你分配的字节数应该是4的整数倍。 以下是数据排列和缓冲区大小的一些技巧： 尽管vImage默认可以有更少的数据排列，但为了性能最优化，所有数据都应该是16字节排列，且rowbytes应该是16的整数倍。 浮点型数据必须至少是4字节，否则某些函数可能会报错。 函数的rowBytes参数不能传入2的幂次方。 缓冲区（buffer）重用机制很多函数在执行任务时，会使用临时缓冲区来存储中间值。在一开始只需创建一次buffer，就可以在多个函数中使用，这样可以节省时间。 如果你没有提供buffer，这些函数会自行创建buffer（当然，使用完后释放）。 如果你只需要调用几次该函数，并且不在意短暂的阻塞，那么让函数自行创建buffer是个明智的选择。 每个使用到临时buffer的函数都有一个src和desc参数（数据类型都是vImage_Buffer）。函数只使用了这些参数的height和width域；忽略data和rowBytes域。 可能的话，应用也应该尝试重用vImage_Buffer数据的data域指向的图像缓冲区。这样可以节省时间，否则还要重新分配并且把原来的buffer抹平。 在实际应用中，要尽可能的避免使用堆和其他可能引起阻塞的操作（比如内存分配）。 适当的使用线程vImage是线程安全的并且可重入。如果你分割了你的图像，你可以使用多线程处理不同的tiles。如果你使用不同的处理器处理不同的tiles，你应选择那些水平方向上不相邻的tiles。否则tile的边缘可能会共享到cache，这有可能导致两个处理器之间耗时的干扰。 在vImage函数工作时，vImage的输出buffer状态是未知的。有可能buffer中的像素数据既不是起始数据也不是结束数据，而是计算过程中的一个中间值。 在OS X 10.4之后，一些vImage函数在内部使用了多线程技术。他们自己做了参数检查和多线程来提高性能。vImage维持了它延迟分配线程缓冲池的风格去做这件事。这些线程一旦被创建就不会被销毁，他们会被重用。被调用的线程要等待上一个线程完成自己的任务，在这之前会被阻塞。使用内部实现了多线程技术的函数是安全的。 线程安全的函数通过锁来保持数据一致性。如果你不希望函数使用锁，你需要设置kvImageDoNotTile标志位来阻止vImage使用多线程和tiling技术。如果你设置了该标志位，你的应用需要自行处理数据tiling和多线程。 把2D核分为1D核如果你使用卷积方法对图像添加滤镜，你可以通过将2D核分裂为两个1D核来提高性能，这样可以使用两次卷积（一个维度可以使用一次）。 当然，你可以将2D核传给vImageConvolve函数。vImage使用2D核来完成9层8加的操作来计算每个像素结果。为了更好的性能，对每个1D卷积滤镜调用一次vImageConvolve函数。核分裂后，vImage通过每个卷积对每个像素执行3层2加的操作，加起来是6层4加。我们注意到将核分裂成两个后，算法复杂度在乘法上节省了1/3，在加法上节省了1/2。对于M×N的内核，处理消耗从M*N骤减到M+N。一个5×5的核分裂后的处理速度加快2.5倍，一个11×11的核分裂后可能加快超过5倍。 注意这个技术在常规情况下是比较慢的。一般在图像特别大，或图像无法存入内存时使用这个技术。 有几种情况下，分裂特别大的滤镜是执行卷积操作的唯一方案。一个总计超过224的滤镜，vImage使用8位的卷积操作下，运行起来会有内存溢出的风险。这个时候，可以采用分裂滤镜来避免溢出，你甚至可以在放大滤镜之前，通过分裂技术向滤镜里添加更多的固定精度的点。这种技术的中间值精度损失程度未知。]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
</search>